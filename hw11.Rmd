---
title: "hw11"
author: "Alexis Maldonado"
date: "11/27/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

2.a
```{r}
library(ISLR2)

Hitters = na.omit(Hitters)
Hitters$Salary = log(Hitters$Salary)
```
2.b
```{r}
train = Hitters[1:200, ]
test = Hitters[201:nrow(Hitters), ]
```

2.c
The final values used for the model were n.trees = 300, interaction.depth
 = 3, shrinkage = 0.01 and n.minobsinnode = 5.
```{r}
library(caret)
library(gbm)

formula <- Salary~.
mygrid <- expand.grid(interaction.depth = seq(1, 5, by = 1),
                        n.trees = c(100,200,300),
                        n.minobsinnode = 5,
                        shrinkage = c(0.001,0.01,0.1))
set.seed(100)
gbm_model <- train(formula, 
                   data = train,
                   method = "gbm",
                   tuneGrid = mygrid,
                   trControl = trainControl(method = "cv",number=10))

```

2.d
In the booested model CAtBat, CRBI, CWalks, and CHits appear to be the most important.
```{r}
boost.Hitters = gbm(Salary~.,data= train, distribution = "gaussian", n.trees=300, interaction.depth = 3, shrinkage = c(0.01)) 
summary(boost.Hitters)
```

2.e
```{r}
yhat.boost = predict(boost.Hitters, newdata= train, n.trees=300)
yhat.boost = yhat.boost[1:length(test$Salary)]
mean((yhat.boost- test$Salary)^2)
```

2.f
20/3 = ~ 7
```{r}
library(randomForest)
rf_model = randomForest(Salary ~ ., data = train, ntree = 500, mtry = 7, importance = TRUE )

yhat.rf = predict(rf_model, newdata = test)
print(mean((yhat.rf - test$Salary)^2))

```

.---------------------------------------------------------


3.a
```{r}
library(tree)
set.seed(1) # so we all get the same x values. 
n = 100
p = 20
all_training_sets = list()
predicted_values <- matrix(NA, nrow = 5, ncol = 1000)

for (j in 1:1000) {
  Xmat = matrix(NA,nrow=n,ncol=p)
  for(i in 1:p){
    Xmat[,i] = rnorm(n)
  }
  
  beta = rep(seq(1,3,length.out=5),4)
  Y = Xmat%*%beta + rnorm(n,0,1)

  train_set = data.frame(Xmat,Y)
  all_training_sets[[j]] = train_set
  
  tree_model = tree(Y ~ ., data = train_set)
  
  if (j <= 5) {
    predicted_values[j, ] = predict(tree_model, newdata = data.frame(matrix(1, nrow = 1, ncol = 20)))
    print(predicted_values[j, 1])
  }
}
```

3.b
```{r}
(mean(predicted_values) - mean(Y))^2
mean((predicted_values - mean(predicted_values))^2)
```

3.c
```{r}
library(randomForest)
set.seed(1) # so we all get the same x values. 
n = 100
p = 20
all_training_sets = list()
predicted_values2 <- matrix(NA, nrow = 5, ncol = 1000)

for (j in 1:1000) {
  Xmat = matrix(NA,nrow=n,ncol=p)
  for(i in 1:p){
    Xmat[,i] = rnorm(n)
  }
  
  beta = rep(seq(1,3,length.out=5),4)
  Y = Xmat%*%beta + rnorm(n,0,1)

  train_set = data.frame(Xmat,Y)
  all_training_sets[[j]] = train_set
  
  rf_model = randomForest(Y ~ ., data = train_set, mtry = 10, ntree = 200)
  
  if (j <= 5) {
    predicted_values2[j, ] = predict(rf_model, newdata = data.frame(matrix(1, nrow = 1, ncol = 20)))
    print(predicted_values2[j, 1])
  }
}
```

3.d
```{r}
(mean(predicted_values2) - mean(Y))^2
mean((predicted_values2 - mean(predicted_values2))^2)
```

3.e
We have an increase in order of magnitude of 177 to 200

3.f
We have a dencrease in order of magnitude of 16 to 3.

3.g
We can see the increasing the bias does lead to a decrease in variance. We see that the random forest tree has alot less variance compared to the single tree.

.---------------------------------------------------------


4.a
```{r}

```

4.b
For each iteration we have the observations closest to their cluster become part of the cluster meaning that each point is the closest it can be to its 'kind' causing there to be less varation since farther observations would not be choosen.


4.c
i.
```{r}
x1 = c(1, 1, 0, 5, 6, 4)
x2 = c(4, 3, 4, 1, 2, 0)
plot(x1, x2,
     main = "True clusters", xlab = "x1", ylab = "x2", pch = 20, cex = 2)
```

ii.
```{r}
set.seed(10)
cl = sample(2, 6, replace = T)
cl
```

iii.
c1 = 
1/3(1+1+4) = 2 (x1)
1/3(4+3+0) = 2 1/3 (x2)

c2 = 
1/3(0+5=6) = 3 2/3 (x1)
1/3(4+1+2) = 2 1/3 (x2)

iv.
Cluster 1 would have obs. 1, 2, and 3
Cluster 2 would have obs. 4, 5, and 6


v.
c1 = 
1/3(1+1+0) = 2/3 (x1)
1/3(4+3+4) = 3 2/3 (x2)

c2 = 
1/3(5+6+4) = 5 (x1)
1/3(1+2+0) = 1 (x2)
I believe mine should not change anymore after calculating the new centroids.

vi.
```{r}
cl2 = c(1,1,1,2,2,2)
plot(x1, x2, col = cl2,
     main = "True clusters", xlab = "x1", ylab = "x2", pch = 20, cex = 2)
```

.---------------------------------------------------------


5.a
```{r}
library("dendextend")
distM = as.dist(matrix(c(0, 0.3, 0.4, 0.7, 0.3, 0, 0.5, 0.8, 0.4, 0.5, 0.0, 0.45, 0.7, 0.8, 0.45, 0.0), nrow = 4))
dendro = as.dendrogram(hclust(distM, method = "complete"))
dendo = color_branches(dendro,k=4) 
plot(dendo)
```

5.b
```{r}
dendro_single = as.dendrogram(hclust(distM, method = "single"))
dendro_single = color_branches(dendro_single,k=4) 
plot(dendro_single)
```

5.c
If you were to make a single cut in A we would get two clusters 1 and 2 would be a cluster and 3 and 4 would be another cluster

5.d
If you were to make a single cut in B we would get two clusters 1, 2, and 3 would be a cluster and 4 alone would be another cluste.